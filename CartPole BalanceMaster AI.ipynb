{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7028ffca-5bd1-47c8-87ec-055556c99e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26.2\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "print(gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1dcaafd-bbe3-4e21-ab9e-bc1c408750c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 begins...\n",
      "Step 1 | State: [ 0.04005907 -0.01885066  0.02853133 -0.02254513] | Action: 1 | Reward: 1.0\n",
      "Step 2 | State: [ 0.03968205  0.17585075  0.02808043 -0.3060913 ] | Action: 0 | Reward: 1.0\n",
      "Step 3 | State: [ 0.04319907 -0.01965986  0.0219586  -0.00468645] | Action: 1 | Reward: 1.0\n",
      "Step 4 | State: [ 0.04280587  0.17514041  0.02186487 -0.29036108] | Action: 0 | Reward: 1.0\n",
      "Step 5 | State: [ 0.04630868 -0.02028639  0.01605765  0.00913679] | Action: 1 | Reward: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6 | State: [ 0.04590295  0.17460164  0.01624039 -0.2784368 ] | Action: 0 | Reward: 1.0\n",
      "Step 7 | State: [ 0.04939498 -0.02074818  0.01067165  0.01932375] | Action: 0 | Reward: 1.0\n",
      "Step 8 | State: [ 0.04898002 -0.21602154  0.01105813  0.3153545 ] | Action: 1 | Reward: 1.0\n",
      "Step 9 | State: [ 0.04465959 -0.02105884  0.01736522  0.02617936] | Action: 1 | Reward: 1.0\n",
      "Step 10 | State: [ 0.04423841  0.17380983  0.0178888  -0.26097447] | Action: 1 | Reward: 1.0\n",
      "Step 11 | State: [ 0.04771461  0.36867192  0.01266932 -0.5479618 ] | Action: 1 | Reward: 1.0\n",
      "Step 12 | State: [ 0.05508805  0.5636136   0.00171008 -0.8366262 ] | Action: 0 | Reward: 1.0\n",
      "Step 13 | State: [ 0.06636032  0.36846834 -0.01502244 -0.54340595] | Action: 0 | Reward: 1.0\n",
      "Step 14 | State: [ 0.07372969  0.1735607  -0.02589056 -0.2554939 ] | Action: 0 | Reward: 1.0\n",
      "Step 15 | State: [ 0.0772009  -0.02118221 -0.03100044  0.02891159] | Action: 1 | Reward: 1.0\n",
      "Step 16 | State: [ 0.07677726  0.17437027 -0.03042221 -0.27338886] | Action: 0 | Reward: 1.0\n",
      "Step 17 | State: [ 0.08026467 -0.02030469 -0.03588999  0.0095457 ] | Action: 1 | Reward: 1.0\n",
      "Step 18 | State: [ 0.07985857  0.1753131  -0.03569907 -0.29424143] | Action: 0 | Reward: 1.0\n",
      "Step 19 | State: [ 0.08336483 -0.01928218 -0.0415839  -0.01302777] | Action: 1 | Reward: 1.0\n",
      "Step 20 | State: [ 0.08297919  0.1764107  -0.04184446 -0.31853548] | Action: 1 | Reward: 1.0\n",
      "Step 21 | State: [ 0.0865074   0.37210286 -0.04821517 -0.62411547] | Action: 0 | Reward: 1.0\n",
      "Step 22 | State: [ 0.09394946  0.17768605 -0.06069747 -0.3469989 ] | Action: 0 | Reward: 1.0\n",
      "Step 23 | State: [ 0.09750318 -0.01652238 -0.06763745 -0.07405681] | Action: 0 | Reward: 1.0\n",
      "Step 24 | State: [ 0.09717273 -0.21061276 -0.06911859  0.1965428 ] | Action: 1 | Reward: 1.0\n",
      "Step 25 | State: [ 0.09296048 -0.01457372 -0.06518773 -0.11711831] | Action: 1 | Reward: 1.0\n",
      "Step 26 | State: [ 0.092669    0.18141875 -0.0675301  -0.4296342 ] | Action: 0 | Reward: 1.0\n",
      "Step 27 | State: [ 0.09629738 -0.01268515 -0.07612278 -0.15898076] | Action: 1 | Reward: 1.0\n",
      "Step 28 | State: [ 0.09604368  0.18343943 -0.0793024  -0.47467417] | Action: 1 | Reward: 1.0\n",
      "Step 29 | State: [ 0.09971246  0.37958646 -0.08879589 -0.7912605 ] | Action: 1 | Reward: 1.0\n",
      "Step 30 | State: [ 0.10730419  0.5758081  -0.10462109 -1.1105051 ] | Action: 0 | Reward: 1.0\n",
      "Step 31 | State: [ 0.11882035  0.3822045  -0.12683119 -0.85239005] | Action: 0 | Reward: 1.0\n",
      "Step 32 | State: [ 0.12646444  0.18901855 -0.143879   -0.6021272 ] | Action: 1 | Reward: 1.0\n",
      "Step 33 | State: [ 0.1302448   0.3858288  -0.15592153 -0.93644685] | Action: 0 | Reward: 1.0\n",
      "Step 34 | State: [ 0.13796139  0.19311412 -0.17465048 -0.696537  ] | Action: 1 | Reward: 1.0\n",
      "Step 35 | State: [ 0.14182366  0.39017257 -0.18858121 -1.0387146 ] | Action: 0 | Reward: 1.0\n",
      "Step 36 | State: [ 0.14962712  0.19798835 -0.2093555  -0.81066394] | Action: 0 | Reward: 1.0\n",
      "Episode 1 ended after 36 steps.\n",
      "Episode 1 total reward: 36.0\n",
      "Episode 2 begins...\n",
      "Step 1 | State: [ 0.04928567 -0.0144271   0.01640888  0.04421261] | Action: 0 | Reward: 1.0\n",
      "Step 2 | State: [ 0.04899713 -0.20978045  0.01729313  0.3420272 ] | Action: 0 | Reward: 1.0\n",
      "Step 3 | State: [ 0.04480151 -0.40514413  0.02413367  0.6401128 ] | Action: 1 | Reward: 1.0\n",
      "Step 4 | State: [ 0.03669863 -0.2103668   0.03693593  0.3551265 ] | Action: 0 | Reward: 1.0\n",
      "Step 5 | State: [ 0.0324913  -0.40599394  0.04403846  0.65922374] | Action: 0 | Reward: 1.0\n",
      "Step 6 | State: [ 0.02437142 -0.6017002   0.05722294  0.9654417 ] | Action: 0 | Reward: 1.0\n",
      "Step 7 | State: [ 0.01233741 -0.79754215  0.07653177  1.2755381 ] | Action: 1 | Reward: 1.0\n",
      "Step 8 | State: [-0.00361343 -0.60347515  0.10204253  1.0077682 ] | Action: 0 | Reward: 1.0\n",
      "Step 9 | State: [-0.01568293 -0.7998004   0.1221979   1.3306726 ] | Action: 1 | Reward: 1.0\n",
      "Step 10 | State: [-0.03167894 -0.60641325  0.14881134  1.0785917 ] | Action: 0 | Reward: 1.0\n",
      "Step 11 | State: [-0.0438072  -0.8031533   0.17038319  1.4140295 ] | Action: 1 | Reward: 1.0\n",
      "Step 12 | State: [-0.05987027 -0.61050236  0.19866377  1.1790882 ] | Action: 1 | Reward: 1.0\n",
      "Episode 2 ended after 12 steps.\n",
      "Episode 2 total reward: 12.0\n",
      "Episode 3 begins...\n",
      "Step 1 | State: [ 0.04758388 -0.03732545 -0.01755924 -0.0193057 ] | Action: 0 | Reward: 1.0\n",
      "Step 2 | State: [ 0.04683737 -0.23219123 -0.01794535  0.26778576] | Action: 0 | Reward: 1.0\n",
      "Step 3 | State: [ 0.04219355 -0.42705256 -0.01258964  0.55475503] | Action: 1 | Reward: 1.0\n",
      "Step 4 | State: [ 0.0336525  -0.2317561  -0.00149454  0.2581323 ] | Action: 1 | Reward: 1.0\n",
      "Step 5 | State: [ 0.02901737 -0.03661285  0.00366811 -0.03502164] | Action: 0 | Reward: 1.0\n",
      "Step 6 | State: [ 0.02828512 -0.23178722  0.00296767  0.25881636] | Action: 0 | Reward: 1.0\n",
      "Step 7 | State: [ 0.02364937 -0.4269514   0.008144    0.55243385] | Action: 1 | Reward: 1.0\n",
      "Step 8 | State: [ 0.01511034 -0.23194477  0.01919268  0.2623279 ] | Action: 1 | Reward: 1.0\n",
      "Step 9 | State: [ 0.01047145 -0.03710196  0.02443924 -0.02424018] | Action: 0 | Reward: 1.0\n",
      "Step 10 | State: [ 0.00972941 -0.23256572  0.02395443  0.27605233] | Action: 0 | Reward: 1.0\n",
      "Step 11 | State: [ 0.0050781  -0.4280211   0.02947548  0.5761932 ] | Action: 1 | Reward: 1.0\n",
      "Step 12 | State: [-0.00348233 -0.23332444  0.04099934  0.2929396 ] | Action: 1 | Reward: 1.0\n",
      "Step 13 | State: [-0.00814882 -0.0388103   0.04685814  0.01346403] | Action: 0 | Reward: 1.0\n",
      "Step 14 | State: [-0.00892502 -0.23457184  0.04712741  0.32055527] | Action: 0 | Reward: 1.0\n",
      "Step 15 | State: [-0.01361646 -0.43033215  0.05353852  0.62772006] | Action: 0 | Reward: 1.0\n",
      "Step 16 | State: [-0.0222231  -0.6261589   0.06609292  0.93677205] | Action: 0 | Reward: 1.0\n",
      "Step 17 | State: [-0.03474628 -0.82210684  0.08482836  1.2494694 ] | Action: 0 | Reward: 1.0\n",
      "Step 18 | State: [-0.05118842 -1.0182074   0.10981775  1.5674723 ] | Action: 1 | Reward: 1.0\n",
      "Step 19 | State: [-0.07155257 -0.82455534  0.1411672   1.3109655 ] | Action: 0 | Reward: 1.0\n",
      "Step 20 | State: [-0.08804367 -1.0211544   0.1673865   1.6442959 ] | Action: 0 | Reward: 1.0\n",
      "Step 21 | State: [-0.10846676 -1.2177935   0.20027243  1.9841143 ] | Action: 0 | Reward: 1.0\n",
      "Episode 3 ended after 21 steps.\n",
      "Episode 3 total reward: 21.0\n",
      "Episode 4 begins...\n",
      "Step 1 | State: [-0.04845782 -0.00324533  0.0330633  -0.04669178] | Action: 0 | Reward: 1.0\n",
      "Step 2 | State: [-0.04852273 -0.1988254   0.03212946  0.25623682] | Action: 1 | Reward: 1.0\n",
      "Step 3 | State: [-0.05249923 -0.00417655  0.0372542  -0.02614132] | Action: 0 | Reward: 1.0\n",
      "Step 4 | State: [-0.05258277 -0.19981241  0.03673137  0.27805904] | Action: 1 | Reward: 1.0\n",
      "Step 5 | State: [-0.05657901 -0.0052332   0.04229255 -0.0028163 ] | Action: 1 | Reward: 1.0\n",
      "Step 6 | State: [-0.05668368  0.1892575   0.04223622 -0.2818612 ] | Action: 1 | Reward: 1.0\n",
      "Step 7 | State: [-0.05289853  0.38375235  0.036599   -0.5609295 ] | Action: 1 | Reward: 1.0\n",
      "Step 8 | State: [-0.04522348  0.5783421   0.02538041 -0.8418609 ] | Action: 1 | Reward: 1.0\n",
      "Step 9 | State: [-0.03365664  0.77310854  0.00854319 -1.1264555 ] | Action: 1 | Reward: 1.0\n",
      "Step 10 | State: [-0.01819447  0.96811754 -0.01398592 -1.4164466 ] | Action: 0 | Reward: 1.0\n",
      "Step 11 | State: [ 0.00116788  0.77317154 -0.04231485 -1.128168  ] | Action: 1 | Reward: 1.0\n",
      "Step 12 | State: [ 0.01663131  0.96882147 -0.06487821 -1.433817  ] | Action: 1 | Reward: 1.0\n",
      "Step 13 | State: [ 0.03600774  1.164681   -0.09355455 -1.746049  ] | Action: 1 | Reward: 1.0\n",
      "Step 14 | State: [ 0.05930136  1.3607337  -0.12847553 -2.066307  ] | Action: 1 | Reward: 1.0\n",
      "Step 15 | State: [ 0.08651604  1.5569087  -0.16980167 -2.3958123 ] | Action: 1 | Reward: 1.0\n",
      "Episode 4 ended after 15 steps.\n",
      "Episode 4 total reward: 15.0\n",
      "Episode 5 begins...\n",
      "Step 1 | State: [ 0.00414818  0.03638074  0.01979475 -0.02905007] | Action: 0 | Reward: 1.0\n",
      "Step 2 | State: [ 0.0048758  -0.1590194   0.01921375  0.26981202] | Action: 0 | Reward: 1.0\n",
      "Step 3 | State: [ 0.00169541 -0.3544102   0.02460999  0.5684926 ] | Action: 0 | Reward: 1.0\n",
      "Step 4 | State: [-0.00539279 -0.5498685   0.03597984  0.8688259 ] | Action: 0 | Reward: 1.0\n",
      "Step 5 | State: [-0.01639017 -0.74546105  0.05335636  1.1726006 ] | Action: 0 | Reward: 1.0\n",
      "Step 6 | State: [-0.03129939 -0.94123447  0.07680838  1.4815221 ] | Action: 1 | Reward: 1.0\n",
      "Step 7 | State: [-0.05012408 -0.747129    0.10643882  1.2137818 ] | Action: 0 | Reward: 1.0\n",
      "Step 8 | State: [-0.06506666 -0.94345117  0.13071446  1.5378325 ] | Action: 1 | Reward: 1.0\n",
      "Step 9 | State: [-0.08393568 -0.75012213  0.1614711   1.2886336 ] | Action: 1 | Reward: 1.0\n",
      "Step 10 | State: [-0.09893812 -0.55737996  0.18724377  1.0505476 ] | Action: 0 | Reward: 1.0\n",
      "Step 11 | State: [-0.11008572 -0.75442487  0.20825472  1.3956773 ] | Action: 0 | Reward: 1.0\n",
      "Episode 5 ended after 11 steps.\n",
      "Episode 5 total reward: 11.0\n",
      "\n",
      "Simulation Summary:\n",
      "Total Episodes: 5\n",
      "Average Reward: 19.00\n",
      "Max Reward: 36.00\n",
      "Min Reward: 11.00\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "num_episodes = 5  \n",
    "max_steps_per_episode = 200  \n",
    "\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]  \n",
    "    cumulative_reward = 0  \n",
    "\n",
    "    print(f\"Episode {episode + 1} begins...\")\n",
    "    for step in range(max_steps_per_episode):\n",
    "        env.render()  \n",
    "\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        print(f\"Step {step + 1} | State: {state} | Action: {action} | Reward: {reward}\")\n",
    "\n",
    "        cumulative_reward += reward\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done or truncated:\n",
    "            print(f\"Episode {episode + 1} ended after {step + 1} steps.\")\n",
    "            break\n",
    "\n",
    "    episode_rewards.append(cumulative_reward)\n",
    "    print(f\"Episode {episode + 1} total reward: {cumulative_reward}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"\\nSimulation Summary:\")\n",
    "print(f\"Total Episodes: {num_episodes}\")\n",
    "print(f\"Average Reward: {np.mean(episode_rewards):.2f}\")\n",
    "print(f\"Max Reward: {np.max(episode_rewards):.2f}\")\n",
    "print(f\"Min Reward: {np.min(episode_rewards):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0663a61-8f06-4087-ad8d-38bf7a7f55ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ended\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca385f09-e097-4aee-969e-1eed95e36c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
